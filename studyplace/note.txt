微服务模块

1.建module
2.改pom
  <!--springboot web组件-->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
        <!--日常通用jar包配置-->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-devtools</artifactId>
            <scope>runtime</scope>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>

---------------------------------------------------
3.写yml
server:
 port:

spring:
  application:
    name: cloud-payment-service

---------------------------------------------------
4.主启动Main
com.study.springcloud.OrderMain80

com.study.springcloud.PaymentMain

@SpringBootApplication
@EnableDiscoveryClient

    public static void main(String[] args) {
        SpringApplication.run(,args);
    }

---------------------------------------------------
5.业务类 dao、service、controller
com.study.springcloud
controller.PaymentController


=============================================================================
服务注册中心（discovery、client）：Eureka(AP)、Zookeeper(CP)、Consul(CP)、Nacos

CAP理论:关注粒度是数据，而不是整体系统设计的
一个分布式系统不可能同时很好的满足一致性、可用性和分区容错性这三个需求，最多只能同时较好的满足两个
Consistency(强一致性)
Availability（可用性）
Partition tolerance（分区容错性）
NoSQL数据库根据CAP理论分成三大类
CA - 单点集群，满足一致性，可用性的系统，通常性能不是特别高
CP - 满足一致性，分区容忍性的系统，通常性能不是特别高
AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些



eureka(已停更)
自我保护模式中，不会注销任何服务实例
底层还是使用httpClient

多个eureka地址，可在host文件弄域名，添加
127.0.0.1 eureka7001.com
127.0.0.1 eureka7002.com

-----------------------------------------------------------------
zookeeper
yml
#8004表示注册到zookeeper服务器的支付服务提供者端口号
server:
  port: 8004
#服务别名----注册zookeeper到注册中心名称
spring:
  application:
    name: cloud-payment-service
  cloud:
    zookeeper:
      connect-string: 127.0.0.1:2181
#      connect-string: 192.168.111.144:2181

------------------------------------------------------------------
consul
下载：https://www.consul.io/downloads.html
添加环境变量：E:\consul
cmd 命令窗口执行：consul agent -dev
consul 自带 UI 界面，打开网址：http://localhost:8500 ，可以看到当前注册的服务界面
cmd 命令窗口执行:consul.exe agent -server ui -bootstrap -client 0.0.0.0 -data-dir="E:\consul" -bind X.X.X.X
其中X.X.X.X为服务器ip,即可使用http://X.X.X.X:8500 访问ui而不是只能使用localhost连接
注意：添加环境变量之后有可能无法启动，需要到consul.exe文件夹下面去执行consul agent -dev
-----------------------------------------------------------------
Nacos(AP/CP 可自由切换)
=============================================================================
服务调用（Ribbon、LoadBalancer）
ribbon-----负载均衡+RestTemplate调用
（spring-cloud-starter-netflix-eureka-client 已集成ribbon不需另加依赖）
RestTemplate----
getForObject:返回对象为响应体重数据转化成的对象，可以理解为json
getForEntity:返回对象为ResponseEntity对象，包含了响应中的一些重要信息，比如响应头、响应状态码、响应体等。



负载均衡算法
轮询算法（取余）：rest接口第几次请求数 % 服务器集群总数量 = 实际调用服务器位置下标， 每次服务重启动后rest接口计数从1开始。

IRule
自己的规则不能放在启动类(@ComponentScan)的包内

Spring Cloud Ribbon 是基于Netflix Ribbon实现的一套客户端 负载均衡工具
主要功能是提供客户端的软件负载均衡算法和服务条用。
Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。
就是在配置文件中列出Load Balance后面所有的机器，Ribbon会自动帮助你基于某种规则（如简单轮询、随机连接等），去连接这些机器。

LB负载均衡：将用户的请求平摊的分配到多个服务商，从而达到系统HA（高可用）。
常见的负载均衡软件：Nginx、LVS,硬件F5等
两种方式LB
集中式LB：在服务的消费方和提供方之间使用独立的LB设施（F5硬件、Nginx），负责把访问请求通过某种策略转发至服务的提供方。
进程内LB：把LB逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。
（Ribbon就属于进程内LB,它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。）

Ribbon本地负责均衡客户端 VS Nginx服务端负载均衡区别
Nginx是服务器负载均衡，客户端所有请求都会交给nginx，然后由nginx实现转发请求。即负载均衡是由服务端实现的。
Ribbon是本地负载均衡，在调用微服务接口时，会在注册中心上获取注册信息服务列表之后缓存到JVM本地，从而在本地实现RPC远程服务调用技术。

Ribbon工作步骤
第一步寻找EurekaServer,它优先选择在同一个区域内负载较少的server。
第二步再根据用户指定的策略（轮询、随机、根据响应时间加权），在server去到的服务注册列表中选择一个地址。

==========================================================================
服务调用2（Feign、OpenFeign）
Feign
是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单。
它的使用方法是定义一个服务接口然后在尚明添加注解。Feign也支持可插拔式的编码器和解码器。
Spring Cloud和Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。
Feign可以与Eureka和Ribbon组合使用以支持负载均衡

旨在使编写Java Http客户端变得更容易。
Ribbon+RestTemplate，利用RestTemplate对http请求的封装处理，形成一套模板化的调用方法。
但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会被针对每个微服务自行封装一些客户端来包装这些依赖服务的调用。
所以，Feign在此基础上做了进一步封装，由它来帮助我们定义和实现依赖服务接口的定义。
在Feign的实现下，我们只需创建一个接口并使用注解的方式来配置它（以前是Dao接口上面标注Mapper注解，，现在是一个微服务接口上面标注一个Feign注解即可）
即可完成对服务提供方的接口绑定，简化了使用Spring cloud Ribbon时。自动封装服务调用客户端的开发量。

Feign集成了Ribbon
利用Ribbon维护了Paymeng的服务列表信息，并且通过轮询实现了客户端的负载均衡。
与Ribbon不同的是，通过Feign只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用

OpenFeign
在Feign的基础上支持SpringMVC的注解，如RequesMapping等。
@FeignClient可以解析SpringMVC的@RequestMapping注解下的接口，并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务

<dependency>
    <groupId>org.springframew ork.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>


=======================================================================
服务降级（Hystrix、resilience4j、sentinel）
Hystrix(已停更)
是一个用于处理分布式系统的 延迟 和 容错 的开源库，在分布式系统里，许多依赖不可避免的会调用失败，如超时、异常等。
Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性
“断路器”本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），
向调用方返回一个符合预期的、可处理的备选响应（FallBack）,而不是长时间的等待或者抛出调用方法无法处理的异常，
这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延、乃至雪崩。

主要功能
1.服务降级 fallback
服务器忙，请稍后再试，不让客户端等待并立刻返回一个友好提示
出现降级的情况：程序运行异常、超时、服务熔断触发服务降级、线程池/信号量打满也会导致服务降级

在service/controller方法（服务端/客服端都可以--一般在客服端controller）使用注解@HystrixCommand可以用fallbackMethod指定处理方法，也可通过@HystrixCommand指定超时（3秒）：
需在启动类添加新注解 @EnableCircuitBreaker
@HystrixCommand(fallbackMethod = "paymentInfo_TimeOutHandler",commandProperties = {
            @HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds",value = "3000")
})
客服端启动类添加注解@EnableHystrix，yml添加 feign.hystrix.enabled : true
全局注解 @DefaultProperties(defaultFallback = "")，没配置的@HystrixCommand,也会调用这个跳转方法,可解决代码膨胀

Service接口可以使用这种方法实现降级且解耦，@FeignClient(value = "CLOUD-PROVIDER-HYSTRIX-PAYMENT（调用服务器）",fallback = PaymentFallbackService.class（需要继承该service接口，错误调用的方法）)
可以让客户端自己调用提示，不至于显示报错页面

2.服务熔断 break （注解也是@HystrixCommand）
保险丝，达到最大访问访问后，直接拒绝访问，拉闸限电，然后调用服务降级的方法返回友好提示
熔断机制：是应对雪崩效应的一种微服务链路保护机制，当链路的某个微服务出错、不可用、响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的信息
当检测到该节点微服务调用响应正常后，恢复调用链路。

熔断类型：
熔断打开：请求不再进行调用当前服务，内部设置时钟一般为MTTR(平均故障处理时间),当打开时长达到所设时钟则进入半熔断状态
熔断关闭：熔断关闭不会对服务进行熔断
熔断半开：部分请求根据规则调用当前服务，如果请求成功且符合规则则认为当前服务恢复正常，关闭熔断

涉及断路器的三个重要参数：快照时间，请求总数阈值，错误百分比阈值
1：快照时间：断路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的10秒。
2：请求总数阈值：在快照时间窗内，必须满足请求总数阈值才有资格熔断。默认为20，意味着在10秒内，如果该hystrix命令的调用次数不足20次，即使所有的请求都超时或其他原因失效，断路器都不会打开。
3：错误百分比阈值：当请求总数在快照时间窗内超过了阈值，比如发生了30次调用，如果在这30次调用中，有15次发生了超时异常，也就是超过了50%的错误百分比，在默认设定50%阈值情况下，这时候就会将断路器打开。

断路器打开后
1.断路器打开后，再有请求调用的时候，将不会用主逻辑，而是直接调用降级fallback,通过断路器，实现了主动地发现错误并降级逻辑切换为主逻辑，减少响应延迟的效果。
2.原来的主逻辑要如何恢复？
hystrix为我们实现了自动恢复功能。
当熔断器打开，对主逻辑进行熔断后，hystrix会启动一个休眠时间窗，在这个时间窗内，降级逻辑是临时的成为主逻辑，
当休眠时间窗到期，断路器将进入半开状态，释放一次请求到原来的主逻辑上，如果此次请求正常返回，那么断路器将继续闭合，
主逻辑回复，如果这次请求依然有问题，断路器继续进入打开状态，休眠时间窗重新计时。

3.服务限流
秒杀高并发等操作，严禁一窝蜂的过来拥挤，大家排队，一秒钟N个，有序进行

接近实时的监控 flowlimit
除了隔离依赖服务外，Hystrix还提供了准实时的调用监控（Hystrix DashBoard）,Hystrix会持续地记录所有通过Hystrix发起的请求的执行信息，
并以统计报表和图形的形式展示给用户，包括每秒执行多少，请求多少成功，多少失败等。
Netfilx通过Hystrix-metrics-event-stream项目实现了对以上指标的监控。
Spring Cloud也提供了Hystrix DashBoard的整合，对监控内容转化成可视化界面。
pom引用，启动服务后可在http://localhost:xxx/hystrix看到相关信息
  <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-netflix-hystrix-dashboard</artifactId>
        </dependency>
监控时pom要有web组件依赖（web、actuator）

======================================================================================
服务网关（Zuul、Zuul2、gateway）

Zuul（路由网关）
netflix,zuul维护,zuul2跳票出不来成品，逐渐废弃
官网：https://github.com/Netflix/zuul/wiki
使用传统的Servlet IO处理模型。
缺点
servlet是一个简单的网络IO模型，在并发不高的情景下这种模型是使用的。
但是高并发，线程数量机会上涨，而线程资源代价是昂贵的（上下文切换，内存消耗大）严重影响请求的处理时间。
在一些简单业务场景下，不希望为每个request分配一个线程，只需要1个或几个线程就能应对极大并发的请求，这种业务场景下servlet模型没有优势。
因此Zuul 1.x是基于servlet之上的一个阻塞式处理模型，即spring实行了处理所有request请求的一个servlet（DispatcherServlet）并有该servlet阻塞式处理
所以Spring cloud Zuul无法摆脱servlet模型的弊端

Zuul和gateway的区别
Spring cloud Finchley正式版之前，推荐网关是Netflix提供的Zuul;
1、Zuul 1.x 是一个基于阻塞I/O的API Gateway
2、Zuul 1.x基于Servlet 2.5使用阻塞框架，它不支持任何长连接（如WebSocket）Zuul的设计模式和Nginx交像，
每次I/O操作都是从工作线程中选择一个执行，请求线程被阻塞到工程线程完成，
但是差别是Nginx用C++实现，Zuul用java实现，而JVM本身会有第一次加载较慢的情况，使得Zuul的性能相对较差。
3、Zuul 2.x理念更先进，想基于Netty非阻塞和支持长连接，但SpringCloud目前还没有整合。Zuul 2.x的性能较Zuul 1.x有较大提升，
在性能方面，官方提供的基准测试，Spring Cloud Gateway的RPS(每秒请求数) 是Zuul的1.6倍。
4、Spring Cloud Gateway 建立在Spring framework 5,Spring boot2.x、Spring WebFlux 、Project Reactor之上，使用非阻塞API.
5、Spring Cloud Gateway 还支持WebSocket，并且与Spring紧密集成拥有更好地开发体验。

-------------------------------------------------------------------------------
WebFlux
传统的web框架，如struts2，springmvc等都是基于servlet API与Servlet容器基础上运行的。
但Servlet3.1之后有了异步非阻塞的支持。
而WebFlux是一个典型非阻塞异步的框架，它的核心是基于Reactor的相关API实现的。
相对于传统web框架来说，它可以运行在诸如Netty，Undertow及支持Servlet3.1的容器上。
非阻塞式+函数式编程(Spring5必须让你使用java8)
SpringWebFlux 是Spring 5.0 引入的新的响应式框架，区别于Spring MVC，它不需要依赖Servlet API，它是完全异步非阻塞的，并且基于Reactor来实现响应式流规范。


gateway（新一代网关、spring自己出，整合了环境Spring framework 5,Spring boot2.x、Spring WebFlux 、Project Reactor）
gateway是zuul 1.x的替代版，基于WebFlux框架实现，WebFlux框架底层则是使用了高性能的Reactor模式通信框架Netty(建议看韩顺平讲解netty)
Spring cloud gateway的目标提供统一的路由方式且基于Filter链的方式提供网关的基本功能，如：安全，监控/指标，限流。
动态路由：能够匹配任何请求属性
可以对指定Predicate(通信)和Filter(过滤器);
集成Hystrix的断路器功能；
集成Spring Cloud服务发现功能；
易于编写的Predicate(通信)和Filter(过滤器);
请求限流功能；
支持路径重写。

gateway三大核心概念
Route(路由)
构建网关的基本模块，它由ID,目标URI,一系列的断言和过滤器组成，如果断言为true则匹配该路由

Predicate(断言-----九种方式，参考9527项目的yml)
参考的是Java8的java.util.function.Predicate
开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由

Gateway包括许多内置的Route Predicate工厂。所有这些predicate都与Http请求的不同属性匹配。多个Route Predicate工厂可以进行组合
Gateway创建Route 对象时，使用RoutePredicateFactory创建Predicate对象,Predicate对象可以赋值给Route。

可在cmd里用curl测试接口（可带cookie  url --cookie "xxxx=xxx"、Header  url -H "xxxx"）

Filter(过滤)
指的是Spring框架中GatewayFilter的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改。

生命周期
pre

post

种类
GatewayFilter(局部--31种，可在官网看)
GlobalFilter（全局--10+种）
配置yml filter  或自定义过滤器

GateWay总体
web请求，通过一些匹配条件，定位到真正的服务节点，并在这个转发过程的前后，进行一些精细化控制。
predicate就是我们的匹配条件；而filter，可理解为一个无所不能的拦截器。
有了这两个元素，再加上目标uri，就可以实现一个具体的路由了。

客户端向Gateway发出请求，然后在Gateway Handler Mapping 中找到与请求相匹配的路由，将其发到Gateway web Handler.
Handler再通过指定的过滤器链来讲请求发送到我们实际的服务执行业务逻辑，然后返回。
过滤器可能会在发送代理请求之前（"pre"）或之后("post")执行业务逻辑。
Filter在"pre"类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等，
在"post"类型的过滤器中可以做响应内容、响应头的修改，日志的输出、就来监控等。

gateway----pom.xml
<dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-gateway</artifactId>
        </dependency>
带eureka.
不能带starter-web，不然启动会报错

gateway配置网关路由的两种方法
在配置文件yml中配置
代码中注入RouteLocator的Bean

动态路由
默认情况下Gateway会根据注册中心注册的服务列表，以注册中心上微服务名为路径创建动态路由进行转发，从而实现动态路由的功能


====================================================================================
服务配置（Config、Nacos）
微服务意味着将单体应用中的业务拆分成一个个子服务，每个服务的粒度相对较小，因此系统中会出现大量的服务。由于每个服务都需要必要的配置信息才能运行，
所以一套集中式的、动态的配置管理设施是必不可少的。
SpringCloud提供了ConfigServer来解决这个问题，每个微服务自带application.yml，上百个配置文件的管理。

SpringCloud Config
为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部设置

怎么用
SpringCloud Config分为服务端和客户端两部分。
服务端(3344)也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并未客户端提供获取配置信息，加密/解密信息等访问接口
客户端(3355)则通过制定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息配置
服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。

功能
1、集中管理配置文件
2、不同环境不同配置，动态化的配置更新，分环境部署，如:dev、test、prod、beta、release
3、运行期间动态调整配置，不再需要在每个服务部署机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息
4、当配置发送变动时，服务不需要重启即可感知到配置的变化并应用新的配置
5、将配置信息以Rest接口的形式暴露----post、curl等都可刷新配置

SpringCloud Config默认使用Git来存储配置文件，也支持SVN和本地文件，最推荐Git，且使用http/https访问的形式。

实践
pom依赖eureka
 <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-config-server</artifactId>
        </dependency>

启动类添加新注解 @EnableConfigServer

规则——记得有-号，遵循官网的例子(label:分支(branch)、name(服务名)、profiles(环境dev、test、prod))
/{label}/{application}-{profile}.yml
master分支
http://config-3344:3344/master/config-dev.yml

/{application}-{profile}.yml
默认为master分支
http://config-3344:3344/config-dev.yml

/{application}/{profile}[/{label}]
http://config-3344:3344/config/dev/master

bootstrap.yml与application.yml的区别
application.yml是用户级的资源配置项
bootstrap.yml是系统级的，优先级更高
Spring Cloud会创建一个“Bootstrap Context”,作为Spring应用‘Application Context’的父上下文。
初始化的时候，‘Bootstrap Context’负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的‘Environment’。

‘Bootstrap’属性有高优先级，默认情况下，它们不会被本地配置覆盖。‘Bootstrap context’和‘Application context’有着不同的约定，
所以新增一个‘bootstrap.yml’文件，保证‘bootstrap context’和‘application context’配置分离

要将Client模块下的application.yml文件改为bootstrap.yml——这很重要。
bootstrap.yml是比application.yml先加载。

动态——手动版
boostrap.yml添加
management:
  endpoints:
    web:
       exposure:
         include: "*"

Controller添加注解@RefreshScope

手动在cmd发送post请求，就可以不重启服务，获取新的修改配置
curl -X POST "HTTP://localhost:3355/actuator/refresh"

===================================================================================
服务总线（Bus、Nacos）
Spring Cloud Bus
用来将分布式系统的节点与轻量级消息系统链接起来的框架，它整合了Java的事件处理机制和消息中间件的功能。
支持两种消息代理：RabbitMQ和Kafka
能管理和传播分布式系统间的消息，像一个个分布式执行器，可用于广播状态更改，事件推送等，也可以当做微服务间的通信通道

为什么称为总线
在微服务架构的系统中，通常会使用轻量级的消息代理来构件一个共用的消息主题，并让系统中所有微服务实例都连接上来。
由于该主题中产生的消息会被所有实例监听和消费，所以称它为消息总线。在总线上的各个实例，都可以方便地广播一些需要让其他连接在该主题上的实例都知道的消息。

基本原理
ConfigClient实例都监听MQ中通一个topic（默认是springCloudBus）。当一个服务刷新数据的时候，它会把这个消息放入到Topic中，
这样其他监听同一个Topic的服务就鞥得到通知，然后去更新自身的配置

RabbitMQ配置
下载地址
Erland安装： http://erlang.org/download/otp_win64_21.3.exe
RabbitMQ安装： https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.14/
RabbitMQ安装插件
rabbitmq-plugins enable rabbitmq_management


bus动态刷新全局的设计思想，设计方案
方法1）利用消息总线触发一个客户端/bus/refresh,而刷新所有客户端的配置
方法2）利用消息总线出发一个服务端ConfigServer的/bus/refresh端点，而刷新所有客户端的配置

方法1不合适——原因
1.打破了微服务的职责单一性，因为微服务本身是业务模块，它不应该承担配置刷新的职责
2.破坏了微服务各节点的对等性。
3.有一定的局限性。如：微服务在迁移时，它的网络地址常常会发生变化，此时想要自动刷新，会增加更多的修改


一、配合config 实现一次修改配置，全局广播通知，处处生效
操作（3步）
1）给服务端（3344）配置中心服务端添加消息总线支持
pom修改
  <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-bus-amqp</artifactId>
        </dependency>

yml修改
#rabbitmq相关配置
rabbitmq:
  host: localhost
  port: 5672
  username: guest
  password: guest

management: #rabbitmq相关配置，暴露bus刷新配置的端点  暴露-刷新，pom文件一定要有actuator
  endpoints:  #暴露bus刷新配置的端点
    web:
      exposure:
        include: 'bus-refresh'

2）给客户端（3355、3366）添加消息总线支持
pom修改
  <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-bus-amqp</artifactId>
        </dependency>

yml修改
跟在spring cloud的最后
  rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest

3）配置完后，使用curl 发post请求给服务端，即可实现一次发送， 处处生效
curl -X POST "http://localhost:3344/actuator/bus-refresh"

二、定点通知
curl -X POST "http://localhost:3344/actuator/bus-refresh/config-client:3355"
config-client是yml配置的application.name


================================================================
SpringCloud Stream 消息驱动
屏蔽底层消息中间件的差异，降低切换成本，统一消息的变成模型

官网
https://www.spring.io/projects/spring-cloud-stream#overview

解决的问题（设计思想）
MQ(消息中间件——ActiveMQ、RabbitMQ、RocketMQ、Kafka) 存在多种中间件
项目中可存在多个中间件,在各种消息中间件技术切换、维护、开发的过程产生困难
标准的MQ操作细节
1.生产者/消费者之间靠消息媒介传递信息内容——Message
2.消息必须走特定的通道——MessageChannel
3.消息通道里的消息如何被消费，谁负责收发处理——消息通道的子接口SubscribableChannel,有MessageHandler消息处理器所订阅、处理
RabbitMQ还有exchange、Kafka有Topic和Partitions分区

Stream是一个构建消息驱动微服务的框架。
Stream可以让我们只需要用一种适配绑定的方式，自动地给我们在各种MQ内切换，不再关注具体MQ的细节
通过定义绑定器Binder做为中间层，实现应用程序与消息中间件细节之间的隔离。
应用程序通过inpus（消费者） 或者 outputs（生产者） 来与 Stream 中 binder对象交互。
通过我们配置来binding（绑定），而Stream的binder对象负责与消息中间件交互。
所以，我们只需要搞清楚如何与Spring Cloud Stream 交互就可以方便使用消息驱动的方式。

通过 Spring Integration 来连接消息代理中间件以实现消息事件驱动。
Stream为一些供应商的消息中间件产品提供了个性化的自动化配置实现，引用了发布-订阅、消费组、分区的三个核心概念
目前仅支持RabbitMQ、Kafka

Stream标准流程套路
Binder——方便的连接中间件，屏蔽差异
Channel——通道，是队列Queue的一种抽象，在消息通讯系统中技术实现存储和转发的媒介，通过Channel对队列进行配置
Source和Sink——从Strem发布消息的就是输出，接受消息就是输入

常用API
@Input注解标识输入通道，通过该输入通道接受到的消息进入应用程序
@Output注解标识输出通道，发布的消息将通过该通道离开应用程序
@StreamListener 监听队列，用于消费者的队列的消息接受
@EnableBinding 指通信道channel和exchange绑定在一起

故障现象：重复消费
导致原因
如果建立多个Consume服务，默认分组group是不同的，组流水号不一样，被认为不同组，可以消费

解决方法
自定义配置分组
自定义分配为同一个组，可以解决重复消费问题。
-------------------
故障现象：消费服务关机时有新消息发送，重启时没有获取关机期间的消息——没有持久化消费数据

解决方法
自定义分组……同上
-------------------
不同的组是可以全面消费（重复消费），通一组内会发生竞争关系，只有其中一个可以消费。

=============================================================================
Spring Cloud Sleuth 分布式请求链路跟踪
解决问题--设计思路
在微服务框架中，一个由客户端发起的请求在后端系统中会经过多个不同的服务节点调用来协同产生最后的请求结果，
每一个前段请求都会形成一条复杂的分布式服务调用链路，链路中的任何一环出现高延时或错误都会引起整个请求最后的失败。

官网：https://github.com/spring-cloud/spring-cloud-sleuth
Spring Cloud Sleuth（负责跟踪）提供了一套完整的服务跟踪的解决方案
在分布式系统中提供追踪解决方案并且兼容支持zipkin（负责展现）

zipkin
下载jar包（-exec.jar）：https://dl.bintray.com/openzipkin/maven/io/zipkin/java/zipkin-server/
本地cmd运行 java -jar zipkin-server-2.12.9-exec.jar
localhost:9411 可以看zipkin监控页面

一条链路通过Trace Id唯一标识，Span标识发起的请求信息，各span 通过parent id关联起来
Trace:类似于树结构的Span集合，表示一条调用链路，存在唯一标识
span:标识调用链路来源，通俗的理解span就是一次请求信息

pom.xml
<dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-zipkin</artifactId>
        </dependency>

application.yml
spring.application.name 下加
  zipkin:
    base-url: http://localhost:9411
  sleuth:
    sampler:
      #采样率值介于 0-1 之间，1表示全部采集
      probability: 1

=====================================================
高级篇
Spring Cloud alibaba（20181.0.31加入spring） 是 Spring Cloud netflix 停更后的一次融合
服务限流降级：默认支持Servlet、Feign、RestTemplate、Dubbo和RocketMQ限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级Metrics监控
服务注册与发现：适配Spring Cloud服务注册与发现标准，默认集成Ribbon的支持。
分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。
消息驱动能力：基于Spring Cloud Stream为微服务应用构建消息驱动能力。
阿里云对象存储:阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。
分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于Cron表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有Worker（schedulerx-client）上执行。

git项目:https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md

----------------------------------------------------------------
Nacos(Naming Confinguration Service)——就是注册中心 + 配置中心的组合 ,之前（Eureka/consul、config、bus）
阿里巴巴开源产品，一个更易于构件云原生应用的动态服务发现、配置管理和服务管理平台
替代Eureka做服务中心，替代config做配置中心
git：https://github.com/alibaba/Nacos
官网：https://nacos.io/zh-cn/

注册中心(参考9001/9002/83)-------------------------------------------------------
cmd打开bin的start.cmd
访问：http://localhost:8848/nacos
账号、密码：nacos

pom.xml
 <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
        </dependency>

CP/AP 随意切换
C是所有节点在同一时间看到的数据是一致的；而A的定义是所有的请求都会受到响应

何时选择使用何种模式？
一般来说，
如果不需要存储服务级别的信息且服务实例是通过nacos-client注册，并能够保持心跳上报，那么就可以选择AP模式。
当前主流的服务如 Spring cloud 和 Dubbo服务，都适用于AP模式，AP模式为了服务的可能性而减弱了一致性，因此AP模式下只支持注册临时实例。
如果需要在服务级别编辑或存储配置信息，那么CP是必须，K8S服务和DNS服务则适用于CP模式。
CP模式下则支持注册持久化实例，此时则是以Raft协议为集群运行模式，该模式下注册实例之前必须先注册服务，如果服务不存在，则会返回错误。

curl -X PUT "$NACOS_SERVER:8848/NACOS/V1/ns/operator/switches?entry=serverMode&value=CP"


配置中心（参考3377）---------------------------------------------------------
项目文件配置
pom.xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
</dependency>

新建两个yml
application.yml-----spring.profile.active
spring:
  profiles:
    active: dev #表示开发环境

bootstrap.yml

spring:
  application:
    name: nacos-config-client
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #Nacos服务注册中心地址
      config:
        server-addr: localhost:8848 #Nacos作为配置中心地址
        file-extension: yaml #指定yaml格式的配置

nacos配置中心新建的文件需要按一下命名
${spring.application.name}-${spring.profile.active}.${spring.cloud.nacos.config.file-extension}
nacos-config-client-dev.yaml
................................................................
配置中心--基础配置
配置中心--分类配置

三种方案加载配置（可搭配使用，做成三级配置）
DataId方案
指定spring.profile.active和配置文件的DataID来使不同环境下读取不同的配置
默认空间 + 默认分组 + 新建dev和test两个DataID
通过spring.profile.active属性就能进行多环境下配置文件的读取

Group方案
通过Group实现环境区分

Namespace方案
新建dev/test的Namespace
回到服务管理-服务列表查看
按照域名配置填写
.................................................................
Nacos集群和持久化配置
三种部署模式
单机模式 - 用于测试和单机试用
集群模式 - 用于生存环境，确保高可用。
多集群模式 - 用于多数据中心场景

集群
默认Nacos使用嵌入式数据库实现数据的存储。所以，如果启动多个默认配置下的Nacos节点，数据存储是存在一致性问题的。
为了解决这个问题，Nacos采用了集中式存储的方式来支持集群化部署，目前只支持Mysql的存储
1.安装数据库，版本要求：5.6.5+
2.初始化mysql数据库，数据库初始化文件：nacos-mysql.sql
3.修改conf/application.properties文件，增加支持mysql数据源配置（目前只支持mysql），添加mysql数据源的url、用户名和密码
spring.datasource.platform=mysql
db.num=1
db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_devtest?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
db.user=root
db.password=root

再以单机模式启动nacos，nacos所有写嵌入式数据库的数据都写到mysql

官网文档：https://nacos.io/zh-cn/docs/deployment.html

--------------...................-------------------------------
Linux(centOS8)
nacos:
nacos/conf
1.把nacos-mysql.sql导入本地数据库

2.application.properties (添加mysql数据库连接)
######################################
spring.datasource.platform=mysql

db.num=1
db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
db.user=root
db.password=root
######################################

2.cluster.conf
#nacos对外服务器地址,example
192.168.189.129:3333
192.168.189.129:4444
192.168.189.129:5555

3.修改startup.sh
在s:之后新增—— p:   —— p) PORT=$OPTARG;;
while getopts ":m:f:s:p:" opt
do
    case $opt in
        m)
            MODE=$OPTARG;;
        f)
            FUNCTION_MODE=$OPTARG;;
        s)
            SERVER=$OPTARG;;
	p)
	    PORT=$OPTARG;;
        ?)
        echo "Unknown parameter"
        exit 1;;
    esac
done

# start
在$JAVA ${JAVA_OPT} 之间新增 -Dserver.port=${PORT}
nohup $JAVA -Dserver.port=${PORT} ${JAVA_OPT} nacos.nacos >> ${BASE_DIR}/logs/start.out 2>&1 &

4.nacos启动
nacos/bin
./start.sh -p 3333
./start.sh -p 4444
./start.sh -p 5555

开放linux（centOS8）1000-6000的端口端口
firewall-cmd --zone=public --add-port=1000-6000/tcp --permanent
刷新防火墙
firewall-cmd --reload

启动/关闭mysql
service mysql start/stop

查看有多少个nacos启动了
ps -ef|grep nacos|grep -v grep |wc -l

nginx:
nginx/conf
nginx.conf(看位置修改，不是复制黏贴，cluster跟nacos的cluster.conf里的ip：port一样)
upstream cluster{
		server 127.0.0.1:3333;
		server 127.0.0.1:4444;
		server 127.0.0.1:5555;
	}

server{
  listen :1111
location / {
    proxy_pass http://cluster;
}
}

nginx/sbin
./nginx -c /opt/nginx/conf/nginx.conf   (opt/nginx为nginx地址)
./nginx -s stop 关闭
./nginx -s reload 重启

完成操作，以后可以远程调用 服务器ip：1111(nginx监听端口)/nacos 登陆nacos

-----------------------------------------------------------------
Sentinel
阿里巴巴开源产品，把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
git:github.com/alibaba/Sentinel
下载：github.com/alibaba/Sentinel/releases

Sentinel分为两部分
核心库（Java客户端） 不依赖任何框架/库，能够运行于所有Java运行时环境，同时对Dubbo /Spring Cloud等框架也有较好的支持
控制台（Dashboard） 基于Spring Boot 开发，打包后可以直接运行，不需要额外的Tomcat等应用容器。
.............................
Sentinel和Hystrix对比
Hystrix
需要程序员手工搭建监控平台
没有一套web界面可以给我们进行更加细粒度化的配置流控、速率控制、服务熔断、服务降级

Sentinel
单独一个组件，可以独立出来
直接界面化的细粒度统一配置
约定>配置>编码
可以用代码进行配置，但是尽量用配置和注解的方式，少写代码
..........................
Sentinel配置
pom.xml
  <!--Spring Cloud alibaba nacos-->
       <dependency>
           <groupId>com.alibaba.cloud</groupId>
           <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
       </dependency>
       <!--Spring cloud alibaba sentinel-datasource-nacos 后续做持久化用到-->
       <dependency>
           <groupId>com.alibaba.csp</groupId>
           <artifactId>sentinel-datasource-nacos</artifactId>
       </dependency>
       <!--Spring cloud alibaba sentinel-->
       <dependency>
           <groupId>com.alibaba.cloud</groupId>
           <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
       </dependency>
       <!--openfeign-->
       <dependency>
           <groupId>org.springframework.cloud</groupId>
           <artifactId>spring-cloud-starter-openfeign</artifactId>
       </dependency>

.............
application.yml

spring:
  application:
    name: cloudalibaba-sentinel-service
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #Nacos服务注册中心地址
    sentinel:
      transport:
        #配置Sentinel dashboard地址
        dashboard: localhost:8080
        #默认8719端口，假如被占用会自动从8719开始依次+1扫描，直至找到未被占用的端口
        port: 8719

management:
  endpoints:
    web:
      exposure:
        include: "*"

登陆地址：http://localhost:8080/

..........................
流控规则（判断物为访问流量）
资源名：唯一名称，默认请求路径
针对来源：Sentinel可以针对调用者进行限流，填写微服务名，默认default（不区分来源）
阈值类型/单机阈值：
    QPS(每秒钟的请求数量)：当调用该api的QPS达到阈值时，进行限流
    线程数：当调用该api的线程数达到阈值的时候，进行限流
是否集群：不需要集群

流控模式：
    直接：api达到限流条件时，直接限流
    关联：当关联的资源达到阈值时，就限流自己
    链路：只记录指定链路上的流量（指定资源从入口资源尽量的流量，如果达到阈值，就进行限流）

流控效果：
    快速失败：直接失败，抛异常
    Warm Up: 根据codeFacotr（冷加载因子，默认3）的值，从阈值/codeFacotr，经过预热时长，才达到设置的QPS阈值
    排队等候

=================================
降级规则（判断物为异常）
RT(平均反应时间，秒级)
    平均反应时间 超出阈值 且 在时间窗口内通过的请求>=5，两个条件同时满足后触发降级
    窗口期过后关闭断路器
    RT最大4900 （更大的需要通过-Dcsp.sentinel.statistic.max.rt=XXX才能生效）

异常比例（秒级）
    QPS >= 5 q却异常比例统计超过阈值时，出发降级；时间窗口结束后，关闭降级

异常数（分钟级）
    异常数（分钟统计）超过阈值时，触发降级；时间窗口结束后，关闭降级
=================================
热点规则

@SentinelResource(value = "testHotKey",blockHandler = "deal_testHotKey")
deal_testHotKey兜底返回方法
控制的是sentinel控制台配置的违规情况，有bloackHandler方法配置的兜底处理。

RuntiemException
int age = 10/0,这个是java运行时出现的运行异常，@SentinelResource不管

总结
@SentinelResource主管配置出错，运行时出错该走异常走异常

================================
系统规则
支持模式：
    Load自适应（尽对Linux|Unix-like机器生效）：系统的load1作为启发指标，进行自适应系统保持。当系统load1超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系统保护（BBR阶段）。系统容量由系统的maxQps * minRt 估算得出（参考值 CPU cores* 2.5）。
    CPU usage(1.5.0+ 版本)： 当系统CPU使用率超过阈值即触发系统保护，单位是毫秒。
    平均RT:当单台机器上所有入口流量的平均RT达到阈值即触发系统保护，单位是毫秒。
    并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。
    入口QPS：当单台机器上所有入口流量的QPS达到阈值即触发系统保护。
..........................
sentinel三个核心api
SphU定义资源
Tracer定义统计
ContextUtil定义了上下文

-------------------------
持久化Sentinel限流配置————针对重启服务，限流配置清空
将限流配置规则持久化到Nacos保存，只要刷新8401某个rest地址，sentinel控制台的流控规则就能看到，只要Nacos里面的配置不删除，针对8401上sentinel上的流控规则持续有效

pom.xml

<dependency>
   <groupId>com.alibaba.csp</groupId>
   <artifactId>sentinel-datasource-nacos</artifactId>
</dependency>

application.yml

spring:
  application:
    name: cloudalibaba-sentinel-service
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #Nacos服务注册中心地址
    sentinel:
      transport:
        #配置Sentinel dashboard地址
        dashboard: localhost:8080
        #默认8719端口，假如被占用会自动从8719开始依次+1扫描，直至找到未被占用的端口
        port: 8719
      datasource:
        dsl:
          nacos:
            server-addr: localhost:8848
            dataId: cloudalibaba-sentinel-service
            groupId: DEFAULT_GROUP
            data-type: json
            rule-type: flow

在nacos上添加配置
DataId为服务名

[
    {
        "resource": "/rateLimit/byUrl",
        "limitApp": "default",
        "grade": 1,
        "count": 1,
        "strategy": 0,
        "controlBehavior":0,
        "clusterMode": false
    }
]
..........................
解决问题
服务雪崩、服务降级、服务熔断（整合ribbo、openfeign）、服务限流


-----------------------------------------------------------------
RocketMQ
Apache RocketMQ 基于Java的高性能、高吞吐量的分布式消息和流计算平台

---------------------------------------------------------------
Dubbo
Apache Dubbo 是一款高性能Java RPC框架。

----------------------------------------------------------------
Seata
阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。

.....................................
分布式事务处理过程的一ID+三组件模型

Transaction ID
XID：全局唯一的事务ID

三个组件的概念
Transaction Coordinator（TC) 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚；
Transaction Manager（TM）:控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议；
Resource Manager(RM): 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚；

步骤
1.TM向TC申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID；
2.XID在微服务调用链路的上下文中传播；
3.RM向TC注册分支事务，将其纳入XID对应全局事务的管辖；
4.TM向TC发起针对XID的全局提交或回滚决议；
5.TC调度XID下管辖的全部分支事务完成提交或回滚请求。

............................................
下载
https://github.com/seata/seata/releases
官网
http://seata.io/zh-cn/

--------------------
修改file.conf
主要修改：自定义事务名称+事务日志存储模式为db+数据库连接信息
vgroupMapping.my_test_tx_group = "default" -> "fsp_tx_group"
mode = "file"  -> "db"

sql建数据库、建表（1.0好像自动建表）
git项目的seata/script/server/db/里面有

修改registry.conf
type = "file" -> "nacos"
serverAddr = "localhost" -> "localhost:8848"

bin目录下启动bat
--------------------
mysql


CREATE TABLE `undo_log`(
	`id` BIGINT(20) NOT NULL AUTO_INCREMENT,
    `branch_id` BIGINT(20) NOT NULL,
    `xid` VARCHAR(100) NOT NULL,
    `context` VARCHAR(128) NOT NULL,
    `rollback_info` LONGBLOB NOT NULL,
    `log_status` INT(11) NOT NULL,
    `log_created` DATETIME NOT NULL,
    `log_modified` DATETIME NOT NULL,
    `ext` VARCHAR(100) DEFAULT NULL,
    PRIMARY KEY(`id`),
    UNIQUE KEY `ux_undo_log`(`xid`,`branch_id`)
    )ENGINE = INNODB AUTO_INCREMENT= 1 DEFAULT CHARSET= utf8;


--------------------------
pom.xml,把版本替换成本地版本
        <!--seata-->
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
            <exclusions>
                <exclusion>
                    <artifactId>seata-all</artifactId>
                    <groupId>io.seata</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>io.seata</groupId>
            <artifactId>seata-all</artifactId>
            <version>1.1.0</version>
        </dependency>

application.yml

spring:
  application:
    name: seata-order-service
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #配置Nacos地址
    alibaba:
      seata:
        #自定义事务组名称需要与seata-server中对应
        tx-service-group: fsp_tx_group
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://localhost:3306/seata_order
    data-username: root
    data-password: root

feign:
  hystrix:
    enabled: false

logging:
  level:
    io:
      seata: info

mybatis:
  mapperLocations: classpath:mapper/*.xml


复制file.conf、registry.conf到项目resources文件夹下


--------------------
本地@Transactional
全局@GlobalTransactional——Seata的分布式交易解决方案

-----------------------------------------------------------------
Alibaba Cloud OSS
阿里云对象存储服务（Object Storage Service），阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。

-----------------------------------------------------------------
Alibaba Cloud SchedulerX
阿里中间件团队开发的一款分布式人物调度产品，支持周期性的任务与固定时间点出发任务。




开发软件的标准
永远要有默认的模板，且用户能自定义内容